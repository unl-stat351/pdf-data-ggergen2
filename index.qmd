---
title: "Lab: PDF Data"
author: "Grey Gergen"
format: html
number-sections: true
number-depth: 2
---

::: callout
You can see the purpose of this assignment as well as the skills and knowledge you should be using and acquiring, in the [Transparency in Learning and Teaching (TILT)](tilt.qmd) document in this repository. The TILT document also contains a checklist for self-reflection that will provide some guidance on how the assignment will be graded.
:::

# Data Source

The University of Nebraska publishes the operating budgets by department on an annual basis on [the UN System business and finance office website](https://nebraska.edu/offices-policies/business-finance/budget-and-planning), with [an archive page that contains budgets from previous fiscal years](https://nebraska.edu/offices-policies/business-finance/budget-and-planning/archive).

With impending system-wide budget cuts, our goal is to extract salary data as well as job position data, to determine how much of the cost growth is attributable to growth in administrator salaries, faculty salaries, staff salaries, and other costs.

I have preemptively included 4 budget reports in this repository, spaced at 5 year intervals, but you are welcome to include more years if you would like to do so. The budget reports are lengthy (each report is between 1350 and 1500 pages, and that is just for city campus -- it does not include IANR, the law school, the dental college, etc.).

# Warming Up

## PDF Format

What type of PDF files are these? Based on the format, what would you have to do to process the PDFs and extract text and/or tabular information (in broad terms)?

```{r}
library(pdftools)
library(stringr)

pdf_info("unl-department-budget-2010-2011-pt1.pdf")$version
pdf_text("unl-department-budget-2010-2011-pt1.pdf") |>
  str_split("\n") |>
  unlist() |>
  head()
```

> Text based format. Read in the pdf files, try and detect a table structe, clean the structure, then 

------------------------------------------------------------------------

## PDF Structure

Take a look at the provided PDFs in your default PDF viewer (acrobat, reader, chrome, xPDF, etc). Do they have a consistent structure across years? Across departments? Make a list of at least 3-5 problems you expect to have to overcome if you scrape data from the PDFs. Include screenshots where it is relevant to do so (similar to Fig 34.4 in the textbook), and make sure your images are included using appropriate markdown syntax, captions, and hyperlinks. Discuss how you might overcome these problems and why the PDF format leads to processing challenges.

------------------------------------------------------------------------

1. Indentations under the description column
2. Subsections under title and job class
3. Monetary values starting at different points

------------------------------------------------------------------------

![Indentations](Indentation.png)
![Subsections](subsections.png)

![Monetary](monetary.png)

> All these instances will be problematic in that they start at different x coordinates than most of the data under the corresponding heading.

------------------------------------------------------------------------

## Plan of Attack

What strategy would you use to read in the budget data to minimize the amount of post-processing you need to do? Explain your reasoning.

------------------------------------------------------------------------

> Detect column headings then read in the data column wise, use x and y coordinates to determine column boundaries.

> Tables in pdfs are not specified objects, they are made using x and y coordinates to place information. By finding the headers of the columns we can then extract the data from below.

------------------------------------------------------------------------

## Acquiring Metadata

Use a PDF library to programmatically examine each PDF. Use a functional approach, and organize the metadata in a table, with one row per file. Do you notice any anomalies or unfamiliar metadata components which might be important? Propose a possible hypothesis for any anomalies you discover, and research/explain any unfamiliar terms in the metadata that you identified.

------------------------------------------------------------------------

```{r}
library(dplyr)
library(purrr)
library(tibble)

files <- list.files(pattern = "\\.pdf$", full.names = TRUE)

metadata <- map_dfr(files, function(f) {
  info <- pdf_info(f)
  
  tibble(
    file = basename(f),
    version = info$version,
    pages = info$pages,
    created = info$created,
    modified = info$modified,
    title = info$keys$Title,
    author = info$keys$Author,
    producer = info$keys$Producer
  )
})

metadata

```

------------------------------------------------------------------------

> The most recet budget was made using Adobe Acrobat (64-bit) while the others used Acrobat Distiller. The Acrobat pdf title was not able to be extracted using pdf_info() while the Distiller pdf files were. This may cause issues when we try to use a functional approach to extract data from the files. 

------------------------------------------------------------------------

## Anomalies and Strategy Adjustments

Considering what you discovered in the previous step, do you need to adjust your strategy for reading in the data? Why or why not? Investigate any differences in metadata values across files, and determine whether or not the variation(s) may pose problems for your analysis.

------------------------------------------------------------------------

```{r}
pdf_text_list <- map(files, pdf_text)
```

------------------------------------------------------------------------

> Even though metadata showed that extracting data may be different for the two types of files, pdf_text_list still allowed us to extract information. The variations should not cause problems.

------------------------------------------------------------------------

# Extracting the Text

Now that you've examined the metadata and prepared a strategy, let's see if we can extract the text from each budget report.

## Identify Relevant Pages

Develop a function that takes the path to a PDF file and identifies which pages have tabular salary data on them (e.g. get a range of pages with the salary information). Use your function to create a table with columns `file_name`, `page_start`, and `page_end`.

------------------------------------------------------------------------

```{r}
get_salary_pages <- function(file_path, keyword = "Department:") {
  pages <- pdf_text(file_path)
  
  hits <- which(map_lgl(pages, ~ grepl(keyword, ., ignore.case = TRUE)))
  
  if (length(hits) == 0) {
    return(tibble(file_name = basename(file_path),
                  page_start = NA,
                  page_end = NA))
  }
  
  tibble(file_name = basename(file_path),
         page_start = min(hits),
         page_end = max(hits))
}

salary_pages_tbl <- map_dfr(files, get_salary_pages)

salary_pages_tbl
```

------------------------------------------------------------------------

## Read in Relevant Text

Develop a function `read_salary_data(file, start, end)` which will read in all of the salary data from the pages with tables, using the 2025-2026 salary report as a guide. You should not generalize to other years yet. Use the `pdf_text` function in `tabulapdf` (R) or the `read_pdf` function in the `tabula-py` package (python).

------------------------------------------------------------------------

```{r}
read_salary_data <- function(file, start, end) {
  pages <- pdf_text(file)[start:end]
  
  salary_lines <- unlist(strsplit(pages, "\n"))
  
  return(salary_lines)
}

salary_2025_2026 <- read_salary_data(
  file = "unl-department-budget-2025-2026.pdf",
  start = 8,
  end = 9
)

salary_2025_2026
```

------------------------------------------------------------------------

## Plan your Approach

What processing steps would you use to get the text vectors from this function into a table? Make a detailed list of the necessary steps. Are there any steps you do not think will be consistently successful or generalizable? Is there information your steps sacrifice to read things in cleanly?

------------------------------------------------------------------------

> 1. Split lines into rows
2. Identify headers
3. create table with info

Empty rows or cells could cause problems with determining which rows line up. Subsections may be sacrificed if they do not have a header. 

------------------------------------------------------------------------

## Would Coordinates Help?

There are other functions in the tabula software which provide the coordinates of each piece of text. How might this make it easier to ensure tables are read in correctly?

------------------------------------------------------------------------

> Can use the functions to get dimensions of each column and possibly row. This would ensure accurate column and row matching

------------------------------------------------------------------------

## Explore Package Documentation

Find a function that will provide coordinates for each text component, and write out the steps you might use to convert this data into a clean tabular format. What challenges will you face?

------------------------------------------------------------------------

> locate_text() will be able to give the coordinates of text.
1. Get the coordinates for text per page
2. Group by rows and columns
3. Clean and format the table
4. Repeat per page

different indenting within the columns like the subsections may cause problems.

------------------------------------------------------------------------

## A Classical Problem

Using either approach, get the salary data for all individuals in the Classics department, across all 4 reports (this should require reading in about 2 pages from each report). Plot the salaries of each individual. Generate a second plot of the total budget for the classics department, split by faculty, administration (the chair), and staff (including student workers. What do you notice?

Your answer should address some of the following questions:

-   How has the budget for Classics changed over the last 15 years?
-   How has the proportional allocation of salaries to faculty, staff, and administration changed?
-   What do you think is driving that change?

Your plots must include appropriate titles and legends, and be well constructed using an appropriate mapping. Each plot should be accompanied by a 2-4 sentence description.

------------------------------------------------------------------------

```{r}
get_pages <- function(file_path, keyword = "Department: Classics") {
  pages <- pdf_text(file_path)
  
  hits <- which(map_lgl(pages, ~ grepl(keyword, ., ignore.case = TRUE)))
  
  if (length(hits) == 0) {
    return(tibble(file_name = basename(file_path),
                  page_start = NA,
                  page_end = NA))
  }
  
  tibble(file_name = basename(file_path),
         page_start = min(hits),
         page_end = max(hits))
}

classics_pages_tbl <- map_dfr(files, get_pages)

classics_2010_2011 <- read_salary_data(
  file = "unl-department-budget-2010-2011-pt1.pdf",
  start = 104,
  end = 105
)

classics_2015_2016 <- read_salary_data(
  file = "unl-department-budget-2015-2016.pdf",
  start = 110,
  end = 111
)

classics_2020_2021 <- read_salary_data(
  file = "unl-department-budget-2020-2021.pdf",
  start = 110,
  end = 111
)

classics_2025_2026 <- read_salary_data(
  file = "unl-department-budget-2025-2026.pdf",
  start = 105,
  end = 106
)

classics_all <- bind_rows(
  tibble(year = "2010-2011", line = classics_2010_2011),
  tibble(year = "2015-2016", line = classics_2015_2016),
  tibble(year = "2020-2021", line = classics_2020_2021),
  tibble(year = "2025-2026", line = classics_2025_2026)
)

classics_totals <- classics_all %>%
  filter(str_detect(line, "TOTAL"))
```

------------------------------------------------------------------------

> Overall observations

------------------------------------------------------------------------

## Quality Control

If you were to process the full set of faculty salary data across all departments, what quality control measures would you use to ensure your functions functioned as expected? Explain your answer and your reasoning.

------------------------------------------------------------------------

> Make sure individual salary totals add up to the department totals. This makes sure the pulling of totals is correct and adding up. Check the salary data to see if it is at least 5 digits so we aren't pulling hourly rate or any of the other numerics.

------------------------------------------------------------------------
